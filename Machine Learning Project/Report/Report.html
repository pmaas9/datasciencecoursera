<!DOCTYPE html>
<html>
<head>
  <title>Practical Machine Learning Project:</title>
  <meta charset="utf-8">
  <meta name="description" content="Practical Machine Learning Project:">
  <meta name="author" content="By: Coursera Student">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  <link rel=stylesheet href="libraries/widgets/bootstrap/css/bootstrap.css"></link>
<link rel=stylesheet href="./assets/css/ribbons.css"></link>

  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
        <slide class="title-slide segue nobackground">
  <hgroup class="auto-fadein">
    <h1>Practical Machine Learning Project:</h1>
    <h2>Predicting Quality of Weight Lifting Exercises from Activity Monitor Data</h2>
    <p>By: Coursera Student<br/>Job: Data Scientist</p>
  </hgroup>
  <article></article>  
</slide>
    

    <!-- SLIDES -->
    <slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>The Goal</h2>
  </hgroup>
  <article data-timings="">
    <p>To classify the quality of weight lifting exercises based on data from activity sensors.</p>

<h2>The Data</h2>

<p>The training data consisted of sensor measurements from 6 male participants, 160 variables observed at 19,622 total time points, including an outcome variable (&quot;classe&quot;) with 5 levels.</p>

<h2>My Approach</h2>

<p>My approach to the problem included the following major steps:</p>

<ol>
<li> Data exploration </li>
<li> Data preprocessing</li>
<li> Model selection and cross-validation</li>
<li> Final model fitting</li>
<li> Prediction on the test data</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>1 &amp; 2. Data Exploration and Data Preprocessing</h2>
  </hgroup>
  <article data-timings="">
    <p>By exploring the data, I identified an number of issues that informed my choices about how to best prepare (preprocess) the dataset for the analysis.</p>

<p><strong>Issue #1:</strong> Many variables (104 of them) were almost entirely missing in the dataset, had all unique values, or all had the same value.<br>
<strong>Decision:</strong> Remove these variables from the analysis dataset (as they are not informative), reducing dataset to 56 variables.</p>

<p><strong>Issue #2:</strong> This still leaves a lot of possible predictors and some are likely to be correlated.<br>
To use them all may cause trouble with model fitting.<br>
<strong>Decision:</strong> Use principle components to reduce the number of predictors. (I used the first 10).</p>

<p><strong>Issue #3:</strong> 19,622 is a lot of observations. Some methods (random forests, bagging, etc.) may be computationally intensive, and may not be feasible to carry out in a reasonable time frame.<br>
<strong>Decision:</strong> Consider using faster methods (multinomial regression model, k-means clustering)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>3. Model Selection and Cross-Validation</h2>
  </hgroup>
  <article data-timings="">
    <p>I wanted to try out a couple possible models to use for prediction.  I decided to implement four <strong>multinomial models</strong> (with different model forms) and a <strong>k-means clustering</strong> method. </p>

<p><strong>How did I select a final model from these different options?</strong></p>

<ul>
<li>I split the training data into 10 different sets (folds).</li>
<li>For each cross-validation set, I fit all the models on the other 9 sets (including preprocessing to get principle components).</li>
<li>I then made predictions for the hold-out cross-validation set.</li>
</ul>

<p>This allowed me to estimate the out-of-sample error rate for the different possible models.  </p>

<p>The <strong>k-means clustering</strong> approach (accuracy = 0.95) dramatically outperformed all of the multinomial models that I examined (best accuracy = 0.63).</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Final Model and Out-of-Sample Error Rate</h2>
  </hgroup>
  <article data-timings="">
    <p>Due to its superior performance in my cross-validated analysis, I selected a <strong>k-means clustering</strong> model, based on the first 10 principle components, to be my <strong>final model</strong>.</p>

<p>Based on the cross-validated performance (fitting the model on 90% of the training data and predicting on the final 10% for 10 folds of the data), I expected my out-of-sample accuracy to be 95%, equating to an <em>expected</em> <strong>out-of-sample error rate = 5%</strong>.</p>

<p>I fit the final model on the entire training dataset and used the model to make predictions on the test set data (n=20).  </p>

<p>By submitting my predictions to the Coursera submission page, I was able to get feedback on the <em>actual</em> out-of-sample error rate of my method.  </p>

<p>I correctly identified 19 of the 20 outcomes, and incorrectly classified 1 of the 20, resulting in an <em>actual</em> <strong>out-of-sample error rate = 5%</strong>.</p>

<hr>

<p>This concludes my report on the analysis.  In the next few slides, I have included my code for the project in case you are interested.  Enjoy!</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>My Code for the Project (All of It!)</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">## Load packages and datasets
library(caret); library(nnet); library(gbm); library(survival); library(randomForest);  
library(e1071); library(rpart)
train = read.table(&quot;pml-training.csv&quot;, header=T, sep=&quot;,&quot;, stringsAsFactors=F)
test = read.table(&quot;pml-testing.csv&quot;, header=T, sep=&quot;,&quot;, stringsAsFactors=F)

# remove variables that are almost entirely missing
pare1 = which(colSums(is.na(train))&gt;19000)
pare2 = which(colSums((train == &quot;&quot;))&gt;19000)
pare = union(pare1, c(pare2, 1, 3:5))
train = train[,-pare]
test = test[, -pare]

### identify numeric variables (so in future can get PCs)
hold=ncol(train)
for(i in 1:ncol(train)){  hold[i] = mode(train[,i]) }
these = which(hold==&quot;numeric&quot;)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Code Continued</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">## Look at the data
dim(train); names(train); summary(train)

## Split Data into Folds for Cross-Validation
fold = createFolds(train$classe, k = 10)

###### Determine Which Method is Best #######

## initialize results holders
multinom_predictions&lt;-rep(NA, nrow(train))
multinom_predictions2&lt;-rep(NA, nrow(train))
multinom_predictions3&lt;-rep(NA, nrow(train))
multinom_predictions4&lt;-rep(NA, nrow(train))
knn_predictions&lt;-rep(NA, nrow(train))
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <article data-timings="">
    <pre><code class="r"># evaluate all models with cross-validation
for(i in 1:10){   data_fit = train[-fold[[i]],]; cross_fit = train[fold[[i]],]
# reduce covariate space with principle components
hold = preProcess(data_fit[,these], method=&quot;pca&quot;, na.rm=T, pcaComp=10)
data_pcs = predict(hold, data_fit[,these]);  data_full = cbind(data_fit[,-these], data_pcs)
cross_pcs = predict(hold, cross_fit[,these]); cross_full = cbind(cross_fit[,-these], cross_pcs)
# tell method to train
multinom_model &lt;-multinom(classe ~ ., data = data_full)
multinom_model2 &lt;-multinom(classe ~ as.factor(user_name) + as.factor(new_window) + PC1 + PC2 +  
        PC3 + PC4 + PC5 + PC6 + PC7 + PC8 + PC9 + PC10, data = data_full)
multinom_model3 &lt;-multinom(classe ~ as.factor(user_name) + as.factor(new_window) + (PC1 + PC2 + 
        PC3 + PC4 + PC5 + PC6 + PC7 + PC8 + PC9 + PC10)*as.factor(user_name), data = data_full)
multinom_model4 &lt;-multinom(classe ~ as.factor(user_name) + as.factor(new_window) + (PC1 + PC2 + 
        PC3 + PC4 + PC5 + PC6 + PC7 + PC8 + PC9 + PC10)^2, data = data_full)
modelKNN &lt;- knn3(classe ~ ., data = data_full, k = 5, prob = TRUE)
# predict on fold held out from fitting
multinom_predictions[fold[[i]]] &lt;- predict(multinom_model,  cross_full)
multinom_predictions2[fold[[i]]] &lt;- predict(multinom_model2,  cross_full)
multinom_predictions3[fold[[i]]] &lt;- predict(multinom_model3,  cross_full)
multinom_predictions4[fold[[i]]] &lt;- predict(multinom_model4,  cross_full)
knn_predictions[fold[[i]]]  &lt;- predict(modelKNN, cross_full, type = &quot;class&quot;)   }
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Code Continued</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">###  Calculate the Cross-Validated Accuracy Rate of All Models
mean( multinom_predictions==as.numeric(train$classe ) )
mean( multinom_predictions2==as.numeric(train$classe ) )
mean( multinom_predictions3==as.numeric(train$classe ) )
mean( multinom_predictions4==as.numeric(train$classe ) )
mean( knn_predictions==as.numeric(train$classe ) )
###### Selected Best Method: KNN clustering! #######
# fit on entire training data
data_fit = train; cross_fit = test
# reduce covariate space with principle components
hold = preProcess(data_fit[,these], thresh=0.95, method=&quot;pca&quot;, na.rm=T, pcaComp=10)
data_pcs = predict(hold, data_fit[,these]); data_full = cbind(data_fit[,-these], data_pcs)
cross_pcs = predict(hold, cross_fit[,these]); cross_full = cbind(cross_fit[,-these], cross_pcs)
# fit final model
final_model &lt;-knn3(classe ~ ., data = data_full, k = 5, prob = TRUE)
# predict for test data
test_predictions&lt;- predict(final_model, cross_full, type = &quot;class&quot;)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Code Continued</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r"># function to write out files for submitting answers
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0(&quot;problem_id_&quot;,i,&quot;.txt&quot;)
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
## prepare my answers for submission
answers = as.character(test_predictions)
## create submission files
pml_write_files(answers)

## THE END!
</code></pre>

<!---## The Goal

My objective was to build a model for classifying the quality of weight lifting exercises based on data from activity sensors.

| My Approach | The Data |
| :-------: | :-------: |
|My approach to the problem included the following major steps:| what |
| 1.  Data exploration                       | hi  |
| 2.  Data preprocessing                     | hi1 |
| 3.  Model selection and cross-validation   | hi2 |
| 4.  Final model fitting                    | hi3 | 
| 5.  Prediction on the test data            | hi4 |--->

  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title='The Goal'>
         1
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=2 title='1 &amp; 2. Data Exploration and Data Preprocessing'>
         2
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=3 title='3. Model Selection and Cross-Validation'>
         3
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=4 title='Final Model and Out-of-Sample Error Rate'>
         4
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=5 title='My Code for the Project (All of It!)'>
         5
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=6 title='Code Continued'>
         6
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=7 title=''>
         7
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=8 title='Code Continued'>
         8
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=9 title='Code Continued'>
         9
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  <script src="libraries/widgets/bootstrap/js/bootstrap.min.js"></script>
<script src="libraries/widgets/bootstrap/js/bootbox.min.js"></script>

  <script>  
  $(function (){ 
    $("#example").popover(); 
    $("[rel='tooltip']").tooltip(); 
  });  
  </script>  
  <!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>